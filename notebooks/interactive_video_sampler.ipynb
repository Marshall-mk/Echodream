{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54d40ba",
   "metadata": {},
   "source": [
    "# Interactive Video Sampling with Echo-Dream\n",
    "\n",
    "This notebook allows you to generate videos using the Echo-Dream model with:\n",
    "1. User-defined conditions (class ID, LVEF, view)\n",
    "2. Custom conditioning frames\n",
    "3. Different sampling modes (diffusion, flow matching)\n",
    "4. Various output formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04234bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Add Echo-Dream to path if needed\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Import Echo-Dream modules\n",
    "from echo.common import load_model, save_as_mp4, save_as_gif, save_as_avi, save_as_img\n",
    "from echo.lvdm.sample import get_conditioning_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47e58e",
   "metadata": {},
   "source": [
    "## Setup Model Paths and Configuration\n",
    "\n",
    "First, we need to set up paths to our models and configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43496bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default paths - adjust these to your setup\n",
    "DEFAULT_CONFIG_PATH = \"../echo/lvdm/configs/default.yaml\"\n",
    "DEFAULT_UNET_PATH = \"../models/unet\"\n",
    "DEFAULT_VAE_PATH = \"../models/vae\"\n",
    "DEFAULT_CONDITIONING_PATH = \"../samples/data/reference_frames\"\n",
    "DEFAULT_OUTPUT_PATH = \"../samples/output/interactive\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(DEFAULT_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34996f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default paths - adjust these to your setup\n",
    "DEFAULT_CONFIG_PATH = \"../echo/lvdm/configs/default.yaml\"\n",
    "DEFAULT_UNET_PATH = \"../experiments/lvdm_cardiacnet_df/checkpoint-100000/unet_ema\"\n",
    "DEFAULT_VAE_PATH = \"/nfs/usrhome/khmuhammad/Echonet/models/vae\"\n",
    "DEFAULT_CONDITIONING_PATH = (\n",
    "    \"/nfs/usrhome/khmuhammad/Echonet/data/latents/cardiacnet/Latents\"\n",
    ")\n",
    "DEFAULT_OUTPUT_PATH = \"../samples/output/interactive\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(DEFAULT_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc81290",
   "metadata": {},
   "source": [
    "## Load Models and Configuration\n",
    "\n",
    "Now let's load the models and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2b264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 100000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNetSpatioTemporalConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded config from ../echo/lvdm/configs/default.yaml\n",
      "‚úÖ Loaded UNet from ../experiments/lvdm_cardiacnet_df/checkpoint-100000/unet_ema\n",
      "‚úÖ Loaded VAE from /nfs/usrhome/khmuhammad/Echonet/models/vae\n",
      "üñ•Ô∏è  Using device: cuda\n",
      "‚úÖ Loaded UNet from ../experiments/lvdm_cardiacnet_df/checkpoint-100000/unet_ema\n",
      "‚úÖ Loaded VAE from /nfs/usrhome/khmuhammad/Echonet/models/vae\n",
      "üñ•Ô∏è  Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "def load_models(\n",
    "    config_path=DEFAULT_CONFIG_PATH,\n",
    "    unet_path=DEFAULT_UNET_PATH,\n",
    "    vae_path=DEFAULT_VAE_PATH,\n",
    "):\n",
    "    # Load config\n",
    "    config = OmegaConf.load(config_path)\n",
    "    print(f\"‚úÖ Loaded config from {config_path}\")\n",
    "\n",
    "    # Load models\n",
    "    unet = load_model(unet_path)\n",
    "    vae = load_model(vae_path)\n",
    "\n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    unet = unet.to(device)\n",
    "    vae = vae.to(device)\n",
    "\n",
    "    # Set models to evaluation mode\n",
    "    unet.eval()\n",
    "    vae.eval()\n",
    "\n",
    "    print(f\"‚úÖ Loaded UNet from {unet_path}\")\n",
    "    print(f\"‚úÖ Loaded VAE from {vae_path}\")\n",
    "    print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "    return config, unet, vae, device\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "config, unet, vae, device = load_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9f763",
   "metadata": {},
   "source": [
    "## Load Conditioning Images/Latents\n",
    "\n",
    "We need to load the conditioning frames that will be used to guide the video generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fa3362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loaded 655 tensor files from /nfs/usrhome/khmuhammad/Echonet/data/latents/cardiacnet/Latents\n"
     ]
    }
   ],
   "source": [
    "from echo.common.datasets import TensorSet, ImageSet, TensorSetv2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def load_conditioning_data(conditioning_path=DEFAULT_CONDITIONING_PATH):\n",
    "    # Detect file extension\n",
    "    files = os.listdir(conditioning_path)\n",
    "    if not files:\n",
    "        raise ValueError(f\"No files found in {conditioning_path}\")\n",
    "\n",
    "    file_ext = files[0].split(\".\")[-1].lower()\n",
    "    if file_ext not in [\"pt\", \"jpg\", \"png\"]:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported file extension: {file_ext}. Must be pt, jpg, or png.\"\n",
    "        )\n",
    "\n",
    "    # Load appropriate dataset\n",
    "    if file_ext == \"pt\":\n",
    "        dataset = TensorSetv2(conditioning_path)\n",
    "        print(f\"üìÅ Loaded {len(dataset)} tensor files from {conditioning_path}\")\n",
    "    else:\n",
    "        dataset = ImageSet(conditioning_path, ext=file_ext)\n",
    "        print(f\"üìÅ Loaded {len(dataset)} {file_ext} images from {conditioning_path}\")\n",
    "\n",
    "    return dataset, file_ext\n",
    "\n",
    "\n",
    "# Load conditioning data\n",
    "conditioning_dataset, file_ext = load_conditioning_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e5fcf0",
   "metadata": {},
   "source": [
    "## Create User Interface\n",
    "\n",
    "Let's create a user interface for controlling the video generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4acf97f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1233580e6bdd4d24856eee0ec3db9377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(Dropdown(description='Sampling:', options=(('Diffusion', 'diffusion'), ('Flow Mat‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8abec082daa418d961043109be99acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generate Video', style=ButtonStyle(), tooltip='Click to generate v‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07d26b89dad409d8f0bfc0e698b6d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_ui():\n",
    "    # Sampling parameters\n",
    "    sampling_mode = widgets.Dropdown(\n",
    "        options=[(\"Diffusion\", \"diffusion\"), (\"Flow Matching\", \"flow_matching\")],\n",
    "        value=\"diffusion\",\n",
    "        description=\"Sampling:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    num_steps = widgets.IntSlider(\n",
    "        min=10,\n",
    "        max=500,\n",
    "        step=10,\n",
    "        value=64,\n",
    "        description=\"Steps:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    guidance_scale = widgets.FloatSlider(\n",
    "        min=1.0,\n",
    "        max=10.0,\n",
    "        step=0.5,\n",
    "        value=1.0,\n",
    "        description=\"Guidance:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # Conditioning parameters\n",
    "    conditioning_type = widgets.Dropdown(\n",
    "        options=[\n",
    "            (\"Class ID\", \"class_id\"),\n",
    "            (\"LVEF Value\", \"lvef\"),\n",
    "            (\"View Type\", \"view\"),\n",
    "        ],\n",
    "        value=\"class_id\",\n",
    "        description=\"Condition:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    # We'll change these based on conditioning type\n",
    "    class_id_value = widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=10,\n",
    "        step=1,\n",
    "        value=3,\n",
    "        description=\"Class ID:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    lvef_value = widgets.IntSlider(\n",
    "        min=10,\n",
    "        max=90,\n",
    "        step=5,\n",
    "        value=50,\n",
    "        description=\"LVEF:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        disabled=True,\n",
    "    )\n",
    "\n",
    "    view_value = widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=5,\n",
    "        step=1,\n",
    "        value=0,\n",
    "        description=\"View ID:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        disabled=True,\n",
    "    )\n",
    "\n",
    "    # Output parameters\n",
    "    num_frames = widgets.IntSlider(\n",
    "        min=32,\n",
    "        max=320,\n",
    "        step=32,\n",
    "        value=192,\n",
    "        description=\"Frames:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    output_format = widgets.SelectMultiple(\n",
    "        options=[\"mp4\", \"gif\", \"jpg\", \"avi\"],\n",
    "        value=[\"mp4\", \"jpg\"],\n",
    "        description=\"Format:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    seed = widgets.IntText(\n",
    "        value=None,\n",
    "        description=\"Seed:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        placeholder=\"Random\",\n",
    "    )\n",
    "\n",
    "    # File selection for conditioning frame\n",
    "    file_options = [\n",
    "        f for f in os.listdir(DEFAULT_CONDITIONING_PATH) if f.endswith(f\".{file_ext}\")\n",
    "    ]\n",
    "\n",
    "    conditioning_file = widgets.Dropdown(\n",
    "        options=file_options,\n",
    "        value=file_options[0] if file_options else None,\n",
    "        description=\"Frame:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    # Generate button\n",
    "    generate_button = widgets.Button(\n",
    "        description=\"Generate Video\",\n",
    "        button_style=\"success\",\n",
    "        tooltip=\"Click to generate video with the selected parameters\",\n",
    "    )\n",
    "\n",
    "    # Output area for preview\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Show/hide controls based on conditioning type\n",
    "    def update_conditioning_ui(change):\n",
    "        if change[\"new\"] == \"class_id\":\n",
    "            class_id_value.disabled = False\n",
    "            lvef_value.disabled = True\n",
    "            view_value.disabled = True\n",
    "        elif change[\"new\"] == \"lvef\":\n",
    "            class_id_value.disabled = True\n",
    "            lvef_value.disabled = False\n",
    "            view_value.disabled = True\n",
    "        elif change[\"new\"] == \"view\":\n",
    "            class_id_value.disabled = True\n",
    "            lvef_value.disabled = True\n",
    "            view_value.disabled = False\n",
    "\n",
    "    conditioning_type.observe(update_conditioning_ui, names=\"value\")\n",
    "\n",
    "    # Show or hide frame preview\n",
    "    def show_frame_preview(change):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            if not change[\"new\"]:\n",
    "                return\n",
    "\n",
    "            frame_path = os.path.join(DEFAULT_CONDITIONING_PATH, change[\"new\"])\n",
    "\n",
    "            try:\n",
    "                if file_ext == \"pt\":\n",
    "                    # Load tensor\n",
    "                    frame = torch.load(frame_path)\n",
    "                    # Normalize for display\n",
    "                    if frame.dim() > 3:  # If it's a video tensor with time dimension\n",
    "                        frame = frame[0]  # Take first frame\n",
    "                    # Convert to numpy for display\n",
    "                    if frame.dim() == 3 and frame.shape[0] in [1, 3]:\n",
    "                        # Format: C x H x W\n",
    "                        frame = frame.permute(1, 2, 0).numpy()\n",
    "                        if frame.shape[2] == 1:  # Grayscale\n",
    "                            frame = frame[:, :, 0]\n",
    "                    frame = (frame * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    # Load image\n",
    "                    frame = np.array(Image.open(frame_path))\n",
    "\n",
    "                plt.figure(figsize=(5, 5))\n",
    "                plt.imshow(frame, cmap=\"gray\" if frame.ndim == 2 else None)\n",
    "                plt.title(\"Selected conditioning frame\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading frame: {e}\")\n",
    "\n",
    "    conditioning_file.observe(show_frame_preview, names=\"value\")\n",
    "\n",
    "    # Layout\n",
    "    sampling_box = widgets.VBox([sampling_mode, num_steps, guidance_scale])\n",
    "    conditioning_box = widgets.VBox(\n",
    "        [conditioning_type, class_id_value, lvef_value, view_value]\n",
    "    )\n",
    "    output_box = widgets.VBox([num_frames, output_format, seed])\n",
    "    frame_box = widgets.VBox([conditioning_file])\n",
    "\n",
    "    # Main layout\n",
    "    ui = widgets.Tab()\n",
    "    ui.children = [sampling_box, conditioning_box, output_box, frame_box]\n",
    "    ui.set_title(0, \"Sampling\")\n",
    "    ui.set_title(1, \"Conditioning\")\n",
    "    ui.set_title(2, \"Output\")\n",
    "    ui.set_title(3, \"Frame\")\n",
    "\n",
    "    # Show initial frame preview\n",
    "    show_frame_preview({\"new\": conditioning_file.value})\n",
    "\n",
    "    return (\n",
    "        ui,\n",
    "        generate_button,\n",
    "        output,\n",
    "        sampling_mode,\n",
    "        num_steps,\n",
    "        guidance_scale,\n",
    "        conditioning_type,\n",
    "        class_id_value,\n",
    "        lvef_value,\n",
    "        view_value,\n",
    "        num_frames,\n",
    "        output_format,\n",
    "        seed,\n",
    "        conditioning_file,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create the UI\n",
    "ui_components = create_ui()\n",
    "ui, generate_button, output = ui_components[0], ui_components[1], ui_components[2]\n",
    "sampling_mode, num_steps, guidance_scale = (\n",
    "    ui_components[3],\n",
    "    ui_components[4],\n",
    "    ui_components[5],\n",
    ")\n",
    "conditioning_type, class_id_value, lvef_value, view_value = (\n",
    "    ui_components[6],\n",
    "    ui_components[7],\n",
    "    ui_components[8],\n",
    "    ui_components[9],\n",
    ")\n",
    "num_frames, output_format, seed, conditioning_file = (\n",
    "    ui_components[10],\n",
    "    ui_components[11],\n",
    "    ui_components[12],\n",
    "    ui_components[13],\n",
    ")\n",
    "\n",
    "# Display the UI\n",
    "display(ui)\n",
    "display(generate_button)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73667214",
   "metadata": {},
   "source": [
    "## Generate Video Function\n",
    "\n",
    "Now let's implement the video generation function that will be triggered by the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76878fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from echo.common import pad_reshape, unpad_reshape, padf, unpadf\n",
    "from torch.utils.data import Subset\n",
    "from einops import rearrange\n",
    "import diffusers\n",
    "from echo.common import FlowMatchingScheduler\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "\n",
    "def generate_video_from_ui(b):\n",
    "    # Clear previous output\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"üöÄ Starting video generation...\")\n",
    "\n",
    "        # Get parameters from UI\n",
    "        curr_sampling_mode = sampling_mode.value\n",
    "        curr_num_steps = num_steps.value\n",
    "        curr_guidance_scale = guidance_scale.value\n",
    "        curr_conditioning_type = conditioning_type.value\n",
    "\n",
    "        # Get the appropriate conditioning value\n",
    "        if curr_conditioning_type == \"class_id\":\n",
    "            conditioning_value = class_id_value.value\n",
    "        elif curr_conditioning_type == \"lvef\":\n",
    "            conditioning_value = lvef_value.value\n",
    "        else:  # view\n",
    "            conditioning_value = view_value.value\n",
    "\n",
    "        curr_num_frames = num_frames.value\n",
    "        curr_output_formats = output_format.value\n",
    "        curr_seed = (\n",
    "            seed.value if seed.value not in [None, \"\"] else int(time.time()) % 1000000\n",
    "        )\n",
    "        curr_conditioning_file = conditioning_file.value\n",
    "\n",
    "        # Print generation parameters\n",
    "        print(f\"üìã Parameters:\")\n",
    "        print(f\"   - Sampling mode: {curr_sampling_mode}\")\n",
    "        print(f\"   - Steps: {curr_num_steps}\")\n",
    "        print(f\"   - Guidance scale: {curr_guidance_scale}\")\n",
    "        print(f\"   - Conditioning: {curr_conditioning_type}={conditioning_value}\")\n",
    "        print(f\"   - Frames: {curr_num_frames}\")\n",
    "        print(f\"   - Output formats: {', '.join(curr_output_formats)}\")\n",
    "        print(f\"   - Seed: {curr_seed}\")\n",
    "        print(f\"   - Conditioning file: {curr_conditioning_file}\")\n",
    "\n",
    "        try:\n",
    "            # Set up the scheduler based on sampling mode\n",
    "            if curr_sampling_mode == \"diffusion\":\n",
    "                scheduler_kwargs = OmegaConf.to_container(config.noise_scheduler)\n",
    "                scheduler_klass_name = scheduler_kwargs.pop(\"_class_name\")\n",
    "                scheduler_klass = getattr(diffusers, scheduler_klass_name, None)\n",
    "                assert scheduler_klass is not None, (\n",
    "                    f\"Could not find scheduler class {scheduler_klass_name}\"\n",
    "                )\n",
    "                scheduler = scheduler_klass(**scheduler_kwargs)\n",
    "            else:  # flow_matching\n",
    "                scheduler = FlowMatchingScheduler(\n",
    "                    num_train_timesteps=config.get(\"num_train_timesteps\", 1000)\n",
    "                )\n",
    "\n",
    "            scheduler.set_timesteps(curr_num_steps)\n",
    "            timesteps = scheduler.timesteps\n",
    "\n",
    "            # Set up generator with seed\n",
    "            generator = torch.Generator(device=device).manual_seed(curr_seed)\n",
    "\n",
    "            # Load the conditioning frame\n",
    "            conditioning_path = os.path.join(\n",
    "                DEFAULT_CONDITIONING_PATH, curr_conditioning_file\n",
    "            )\n",
    "\n",
    "            if file_ext == \"pt\":\n",
    "                latent_cond_images = torch.load(conditioning_path).to(device)\n",
    "                if latent_cond_images.dim() == 5:  # B, C, C, H, W (TensorSetv2 format)\n",
    "                    latent_cond_images = latent_cond_images.squeeze(1)\n",
    "            else:  # jpg, png\n",
    "                # Load image and convert to tensor\n",
    "                image = Image.open(conditioning_path).convert(\"RGB\")\n",
    "                transform = transforms.Compose(\n",
    "                    [\n",
    "                        transforms.Resize(\n",
    "                            (config.unet.sample_size, config.unet.sample_size)\n",
    "                        ),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.5], [0.5]),\n",
    "                    ]\n",
    "                )\n",
    "                image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "                # Project to latent space\n",
    "                with torch.no_grad():\n",
    "                    latent_cond_images = (\n",
    "                        vae.encode(image_tensor).latent_dist.sample()\n",
    "                        * vae.config.scaling_factor\n",
    "                    )\n",
    "\n",
    "            # Ensure batch dimension\n",
    "            if latent_cond_images.dim() == 3:  # C, H, W\n",
    "                latent_cond_images = latent_cond_images.unsqueeze(0)  # 1, C, H, W\n",
    "\n",
    "            # Set up dimensions\n",
    "            B = 1  # Just generate one video\n",
    "            C = config.unet.out_channels\n",
    "            H, W = config.unet.sample_size, config.unet.sample_size\n",
    "            T = config.unet.num_frames\n",
    "\n",
    "            # Stitching parameters\n",
    "            NT = curr_num_frames\n",
    "            if NT > T:\n",
    "                OT = T // 2  # overlap\n",
    "                TR = (NT - T) / (T - OT) + 1\n",
    "                TR = int(TR + 0.999)  # ceiling\n",
    "            else:\n",
    "                OT = 0\n",
    "                TR = 1\n",
    "                NT = T\n",
    "\n",
    "            print(\n",
    "                f\"üé¨ Generating video with dimensions: B={B}, C={C}, T={NT}, H={H}, W={W}\"\n",
    "            )\n",
    "\n",
    "            # Prepare latent noise\n",
    "            latents = torch.randn((B, C, NT, H, W), device=device, generator=generator)\n",
    "\n",
    "            # Get conditioning vector\n",
    "            dtype = torch.float\n",
    "            conditioning = get_conditioning_vector(\n",
    "                curr_conditioning_type, conditioning_value, B, device, dtype\n",
    "            )\n",
    "\n",
    "            # Repeat conditioning for temporal stitching if needed\n",
    "            conditioning = (\n",
    "                conditioning.repeat_interleave(TR, dim=0) if TR > 1 else conditioning\n",
    "            )\n",
    "\n",
    "            # Format input/output functions\n",
    "            format_input = (\n",
    "                pad_reshape\n",
    "                if config.unet._class_name == \"UNetSpatioTemporalConditionModel\"\n",
    "                else padf\n",
    "            )\n",
    "            format_output = (\n",
    "                unpad_reshape\n",
    "                if config.unet._class_name == \"UNetSpatioTemporalConditionModel\"\n",
    "                else unpadf\n",
    "            )\n",
    "\n",
    "            # Expand conditioning frame to video latents\n",
    "            latent_cond_images = latent_cond_images[:, :, None, :, :].repeat(\n",
    "                1, 1, NT, 1, 1\n",
    "            )\n",
    "\n",
    "            # Forward kwargs setup\n",
    "            forward_kwargs = {\"timestep\": -1}\n",
    "\n",
    "            if config.unet._class_name == \"UNetSpatioTemporalConditionModel\":\n",
    "                dummy_added_time_ids = torch.zeros(\n",
    "                    (B * TR, config.unet.addition_time_embed_dim), device=device\n",
    "                )\n",
    "                forward_kwargs[\"added_time_ids\"] = dummy_added_time_ids\n",
    "\n",
    "            if curr_conditioning_type == \"text\":\n",
    "                forward_kwargs[\"encoder_hidden_states\"] = conditioning\n",
    "            else:\n",
    "                forward_kwargs[\"encoder_hidden_states\"] = conditioning\n",
    "\n",
    "            print(\"‚è≥ Starting generation loop...\")\n",
    "\n",
    "            # Denoising loop\n",
    "            with torch.no_grad():\n",
    "                for i, t in enumerate(timesteps):\n",
    "                    if i % 10 == 0 or i == len(timesteps) - 1:\n",
    "                        print(f\"   Step {i + 1}/{len(timesteps)}\")\n",
    "\n",
    "                    forward_kwargs[\"timestep\"] = t\n",
    "\n",
    "                    # Prepare model input\n",
    "                    latent_model_input = scheduler.scale_model_input(\n",
    "                        latents, timestep=t\n",
    "                    )\n",
    "                    latent_model_input = torch.cat(\n",
    "                        (latent_model_input, latent_cond_images), dim=1\n",
    "                    )  # B x 2C x T x H x W\n",
    "\n",
    "                    # Handle classifier-free guidance\n",
    "                    use_guidance = curr_guidance_scale > 1.0\n",
    "\n",
    "                    if use_guidance and curr_sampling_mode == \"diffusion\":\n",
    "                        # Create unconditional input\n",
    "                        uncond_kwargs = forward_kwargs.copy()\n",
    "                        uncond_kwargs[\"encoder_hidden_states\"] = torch.zeros_like(\n",
    "                            conditioning\n",
    "                        )\n",
    "\n",
    "                        # Format inputs\n",
    "                        latent_model_input, padding = format_input(\n",
    "                            latent_model_input, mult=3\n",
    "                        )\n",
    "\n",
    "                        # Stitching for conditional prediction\n",
    "                        inputs = torch.cat(\n",
    "                            [\n",
    "                                latent_model_input[:, r * (T - OT) : r * (T - OT) + T]\n",
    "                                for r in range(TR)\n",
    "                            ],\n",
    "                            dim=0,\n",
    "                        )\n",
    "\n",
    "                        # Conditional and unconditional predictions\n",
    "                        noise_pred_cond = unet(inputs, **forward_kwargs).sample\n",
    "                        noise_pred_uncond = unet(inputs, **uncond_kwargs).sample\n",
    "\n",
    "                        # Apply guidance\n",
    "                        outputs_cond = torch.chunk(noise_pred_cond, TR, dim=0)\n",
    "                        outputs_uncond = torch.chunk(noise_pred_uncond, TR, dim=0)\n",
    "\n",
    "                        noise_predictions = []\n",
    "                        for r in range(TR):\n",
    "                            cond_chunk = (\n",
    "                                outputs_cond[r] if r == 0 else outputs_cond[r][:, OT:]\n",
    "                            )\n",
    "                            uncond_chunk = (\n",
    "                                outputs_uncond[r]\n",
    "                                if r == 0\n",
    "                                else outputs_uncond[r][:, OT:]\n",
    "                            )\n",
    "                            guided_chunk = uncond_chunk + curr_guidance_scale * (\n",
    "                                cond_chunk - uncond_chunk\n",
    "                            )\n",
    "                            noise_predictions.append(guided_chunk)\n",
    "\n",
    "                        noise_pred = torch.cat(noise_predictions, dim=1)\n",
    "                    else:\n",
    "                        # Standard prediction without guidance\n",
    "                        latent_model_input, padding = format_input(\n",
    "                            latent_model_input, mult=3\n",
    "                        )\n",
    "\n",
    "                        inputs = torch.cat(\n",
    "                            [\n",
    "                                latent_model_input[:, r * (T - OT) : r * (T - OT) + T]\n",
    "                                for r in range(TR)\n",
    "                            ],\n",
    "                            dim=0,\n",
    "                        )\n",
    "\n",
    "                        noise_pred = unet(inputs, **forward_kwargs).sample\n",
    "                        outputs = torch.chunk(noise_pred, TR, dim=0)\n",
    "\n",
    "                        noise_predictions = []\n",
    "                        for r in range(TR):\n",
    "                            noise_predictions.append(\n",
    "                                outputs[r] if r == 0 else outputs[r][:, OT:]\n",
    "                            )\n",
    "\n",
    "                        noise_pred = torch.cat(noise_predictions, dim=1)\n",
    "\n",
    "                    noise_pred = format_output(noise_pred, pad=padding)\n",
    "\n",
    "                    # Update latents\n",
    "                    if curr_sampling_mode == \"diffusion\":\n",
    "                        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "                    else:  # flow_matching\n",
    "                        dt = 1.0 / (len(timesteps) - 1)\n",
    "                        latents = latents - noise_pred * dt\n",
    "\n",
    "            print(\"üéâ Finished generation! Decoding with VAE...\")\n",
    "\n",
    "            # Decode with VAE\n",
    "            latents = latents / vae.config.scaling_factor\n",
    "\n",
    "            # Process in chunks to save memory\n",
    "            latents = rearrange(latents, \"b c t h w -> (b t) c h w\")\n",
    "            chunk_size = 16  # Process 16 frames at a time\n",
    "            video_chunks = []\n",
    "\n",
    "            for i in range(0, latents.shape[0], chunk_size):\n",
    "                chunk = latents[i : i + chunk_size].to(device)\n",
    "                with torch.no_grad():\n",
    "                    video_chunk = vae.decode(chunk).sample\n",
    "                video_chunks.append(video_chunk.cpu())\n",
    "\n",
    "            video = torch.cat(video_chunks, dim=0)  # (B*T) x C x H x W\n",
    "\n",
    "            # Format output\n",
    "            video = rearrange(video, \"(b t) c h w -> b t h w c\", b=B)\n",
    "            video = (video + 1) * 128\n",
    "            video = video.clamp(0, 255).to(torch.uint8)[0]  # Remove batch dimension\n",
    "\n",
    "            print(\n",
    "                f\"üìπ Video generated! Shape: {video.shape}, range: [{video.min()}, {video.max()}]\"\n",
    "            )\n",
    "\n",
    "            # Create unique ID for this video\n",
    "            video_id = f\"video_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "            # Save video in requested formats\n",
    "            saved_paths = []\n",
    "\n",
    "            # Create subdirectories for formats if they don't exist\n",
    "            for fmt in curr_output_formats:\n",
    "                os.makedirs(os.path.join(DEFAULT_OUTPUT_PATH, fmt), exist_ok=True)\n",
    "\n",
    "            if \"mp4\" in curr_output_formats:\n",
    "                mp4_path = os.path.join(DEFAULT_OUTPUT_PATH, \"mp4\", f\"{video_id}.mp4\")\n",
    "                save_as_mp4(video, mp4_path)\n",
    "                saved_paths.append(mp4_path)\n",
    "\n",
    "            if \"avi\" in curr_output_formats:\n",
    "                avi_path = os.path.join(DEFAULT_OUTPUT_PATH, \"avi\", f\"{video_id}.avi\")\n",
    "                save_as_avi(video, avi_path)\n",
    "                saved_paths.append(avi_path)\n",
    "\n",
    "            if \"gif\" in curr_output_formats:\n",
    "                gif_path = os.path.join(DEFAULT_OUTPUT_PATH, \"gif\", f\"{video_id}.gif\")\n",
    "                save_as_gif(video, gif_path)\n",
    "                saved_paths.append(gif_path)\n",
    "\n",
    "            if \"jpg\" in curr_output_formats:\n",
    "                jpg_dir = os.path.join(DEFAULT_OUTPUT_PATH, \"jpg\", video_id)\n",
    "                save_as_img(video, jpg_dir, ext=\"jpg\")\n",
    "                saved_paths.append(jpg_dir)\n",
    "\n",
    "            # Display the video\n",
    "            if \"mp4\" in curr_output_formats:\n",
    "                display(\n",
    "                    HTML(f\"\"\"\n",
    "                <div style=\"display: flex; justify-content: center;\">\n",
    "                    <video width=\"320\" height=\"320\" controls autoplay loop>\n",
    "                        <source src=\"{mp4_path}\" type=\"video/mp4\">\n",
    "                        Your browser does not support the video tag.\n",
    "                    </video>\n",
    "                </div>\n",
    "                \"\"\")\n",
    "                )\n",
    "            elif \"gif\" in curr_output_formats:\n",
    "                display(Image.open(gif_path))\n",
    "\n",
    "            print(\"\\n‚úÖ Generated video saved to:\")\n",
    "            for path in saved_paths:\n",
    "                print(f\"   - {path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating video: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "# Attach the generate function to the button\n",
    "generate_button.on_click(generate_video_from_ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713f686",
   "metadata": {},
   "source": [
    "## Advanced Options\n",
    "\n",
    "You can customize the generation process further by modifying the default settings below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0294bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model paths if needed\n",
    "def reload_models(config_path=None, unet_path=None, vae_path=None):\n",
    "    global config, unet, vae, device\n",
    "    config_path = config_path or DEFAULT_CONFIG_PATH\n",
    "    unet_path = unet_path or DEFAULT_UNET_PATH\n",
    "    vae_path = vae_path or DEFAULT_VAE_PATH\n",
    "\n",
    "    config, unet, vae, device = load_models(config_path, unet_path, vae_path)\n",
    "\n",
    "\n",
    "# Reload conditioning data from a different location\n",
    "def reload_conditioning_data(conditioning_path=None):\n",
    "    global conditioning_dataset, file_ext\n",
    "    conditioning_path = conditioning_path or DEFAULT_CONDITIONING_PATH\n",
    "    conditioning_dataset, file_ext = load_conditioning_data(conditioning_path)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# reload_models(unet_path=\"/path/to/your/custom/unet\")\n",
    "# reload_conditioning_data(\"/path/to/your/custom/conditioning/frames\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "echosyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
