wandb_group: "triplet_cardiac_asd"
output_dir: /nfs/usrhome/khmuhammad/EchoPath/experiments/triplet_cardiac_asd

vae_path: /nfs/usrhome/khmuhammad/EchoPath/models/vae

globals:
    target_fps: 32
    target_nframes: 64
    outputs: ["image", "class_id", "key_frames"] #, "class_id"

datasets:
    - name: CardiacNetLatent
      active: true
      params:
        root: /nfs/usrhome/khmuhammad/EchoPath/data/latents/cardiac_asd
        target_fps: ${globals.target_fps}
        target_nframes: ${globals.target_nframes}
        target_resolution: 14 # emb resolution
        outputs: ${globals.outputs}

unet:
    _class_name: UNet2DConditionModel
    sample_size: 14 # actual size is 16
    in_channels: 12 # 4 for each key frame
    out_channels: 12 # 4 for each key frame
    center_input_sample: false
    time_embedding_type: positional
    freq_shift: 0
    flip_sin_to_cos: true
    down_block_types: 
        - AttnDownBlock2D
        - AttnDownBlock2D
        - AttnDownBlock2D
        - DownBlock2D
    up_block_types: 
        - UpBlock2D
        - AttnUpBlock2D 
        - AttnUpBlock2D
        - AttnUpBlock2D
    block_out_channels: 
        - 128
        - 256
        - 256
        - 512
    layers_per_block: 2
    mid_block_scale_factor: 1
    downsample_padding: 1
    dropout: 0.0
    act_fn: silu
    cross_attention_dim : 1 # 768 # for text encoder
    #encoder_hid_dim: 1 # 768 # for text encoder
    #encoder_hid_dim_type: text_proj # for text encoder
    norm_eps: 1e-05

noise_scheduler:
    _class_name: DDPMScheduler
    num_train_timesteps: 1000
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: linear # linear, scaled_linear, or squaredcos_cap_v2
    variance_type: fixed_small # fixed_small, fixed_small_log, fixed_large, fixed_large_log, learned or learned_range
    clip_sample: true
    clip_sample_range: 4.0 # default 1 
    prediction_type: v_prediction # epsilon, sample, v_prediction
    thresholding: false # do not touch
    dynamic_thresholding_ratio: 0.995 # unused
    sample_max_value: 1.0 # unused
    timestep_spacing: "leading" #
    steps_offset: 0 # unused

training_mode: diffusion
train_batch_size: 256
dataloader_num_workers: 16
max_train_steps: 10000
training_conditioning_type: class_id # class_id, text, or none

learning_rate: 3e-4
lr_warmup_steps: 500
scale_lr: false
lr_scheduler: constant
use_8bit_adam: false
gradient_accumulation_steps: 1

noise_offset: 0.0
drop_conditionning: 0.1

gradient_checkpointing: false
use_ema: true
enable_xformers_memory_efficient_attention: false
allow_tf32: true

adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 1e-2
adam_epsilon: 1e-08
max_grad_norm: 1.0

logging_dir: logs
mixed_precision: "fp16" # "no", "fp16", "bf16"

validation_timesteps: 128
validation_fps: ${globals.target_fps}
validation_frames: ${globals.target_nframes}
validation_count: 4 # defines the number of samples
validation_guidance: 1.0
validation_steps: 100
validation_conditioning_type: class_id # class_id, text, or none

report_to: wandb
checkpointing_steps: 100 #10000 # ~3/hour
checkpoints_total_limit: 100 # no limit
resume_from_checkpoint: latest
tracker_project_name: echo-path

seed: 42

text_encoder_path: openai/clip-vit-large-patch14
pretrained_model_name_or_path: openai/clip-vit-large-patch14
tokenizer_path: openai/clip-vit-large-patch14
train_text_encoder: False
num_classes: 2 # number of classes, used for class_id conditioning
class_id_to_name:
    0: "ASD"
    1: "Non-ASD"
class_id_to_index:
    ASD: 0
    Non-ASD: 1