# Latent Video Diffusion Model

The Latent Video Diffusion Model (LVDM) is responsible for animating the latent representation of a heart generated by the Latent Image Diffusion Model (LIDM). The LVDM is trained on the VAE-encoded videos. We condition it on an encoded frame and a class index, and train it to reconstruct the video corresponding to that frame and class.

During inference, it can animate any heart, real or synthetic, by conditioning on the latent representation of the heart and the desired pathology (class).

## 1. Activate the environment

First, activate the echosyn environment.

```bash
conda activate echosyn
```

## 2. Data preparation
Follow the instruction in the [Data preparation](../../README.md#data-preparation) to prepare the data for training. Here, you need the VAE-encoded videos.

## 3. Train the LVDM

Once the environment is set up and the data is ready, you can train the LVDM with the following command:

```bash
CUDA_VISIBLE_DEVICES='0,1,2,3' accelerate launch  \
	--num_processes 4  \
	--multi_gpu   \
	--mixed_precision fp16 \
	-m  echo.lvdm.train  \
	--config echo/lvdm/configs/cardiacnet.yaml  \
	--training_mode diffusion \
	--conditioning_type class_id
```

## 4. Sample from the LVDM

Once the LVDM is trained, you can sample from it with the following command:

```bash

CUDA_VISIBLE_DEVICES='0' python -m echo.lvdm.sample  \
    --config echo/lvdm/configs/cardiacnet.yaml   \
    --unet experiments/lvdm_cardiacnet/checkpoint-87000/unet_ema   \
    --vae Echodream/models/vae   \
    --conditioning samples/lidm_cardiacnet/privacy_compliant_latents  \
    --output samples/lvdm_cardiacnet  \
    --num_samples 655    \
    --batch_size 24    \
    --num_steps 256     \
    --save_as mp4,jpg    \
    --frames 192 \
    --sampling_mode diffusion \
    --conditioning_type csv  \
    --condition_guidance_scale 5 \
    --frame_guidance_scale 1  \
    --use_separate_guidance
```

This will generate 655 videos of 192 frames, conditioned on the latent synthetic and privacy-compliant representation of the heart and the same class indices used in synthesizing the latent images. The videos will be saved in the `samples/lvdm_cardiacnet/mp4` directory.

## 5. Evaluate the LVDM

To evaluate the LIDMs, we use the FID, FVD16, and IS scores. To do so, we need to generate 655 videos with 192 frames for the cardiacnet dataset. The output MUST include the jpg format.
The samples are compared to the real samples, which are generated in the [Data preparation](../../README.md#data-preparation) step.

Then, to evaluate the synthetic videos, run the following commands:

```bash
cd external/stylegan-v

python src/scripts/calc_metrics_for_dataset.py \
    --real_data_path ../../data/reference/cardiacnet \
    --fake_data_path ../../samples/lvdm_cardiacnet/jpg \
    --mirror 0 --gpus 1 --resolution 112 \
    --metrics fvd2048_16f,fid50k_full,is50k >> "../../samples/lvdm_cardiacnet/metrics.txt"
```

## 6. Save the LVDM for later use

Once the LVDM is trained, you can save it for later use with the following command:

```bash
mkdir -p models/lvdm_cardiacnet; cp -r experiments/lvdm_cardiacnet/checkpoint-87000/unet_ema/* models/lvdm_cardiacnet/; cp experiments/lvdm_cardiacnet/config.yaml models/lvdm/
```

This will save the selected ema version of the model, ready to be loaded in any other script as a standalone model.